---
title: "8106hw1"
author: "Ze Li zl2746"
output: pdf_document
---

```{r library}
library(ISLR)
library(glmnet)
library(caret)
library(tidymodels)
library(corrplot)
library(ggplot2)
library(pls)
```


```{r data}
train=read.csv("/Users/zeze/Library/Mobile Documents/com~apple~CloudDocs/2024/24S BIST P8106 DS II/hw1/housing_training.csv")
test=read.csv("/Users/zeze/Library/Mobile Documents/com~apple~CloudDocs/2024/24S BIST P8106 DS II/hw1/housing_test.csv")
train = na.omit(train)
test = na.omit(test)

# train
# matrix of predictors (glmnet uses input matrix)
x <- model.matrix(Sale_Price ~ ., train)[,-1]
# vector of response
y <- train[, "Sale_Price"]
x_test <- model.matrix(Sale_Price ~ ., test)[,-1]
y_test <- test[, "Sale_Price"]
```

**(a) Fit a lasso model on the training data. Report the selected tuning parameter and the test error. **

```{r lasso}
ctrl1 <- trainControl(method = "cv", number = 10)

set.seed(2024)
lasso.fit <- train(Sale_Price ~ .,
                   data = train,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1, 
                                          lambda = exp(seq(8, -2, length = 51))),
                   trControl = ctrl1)

# visualization
plot(lasso.fit, xTrans = log)

# tuning parameter
lasso.fit$bestTune

# coefficients in the final model
coef(lasso.fit$finalModel, lasso.fit$bestTune$lambda)

# test error
lasso.predict <- predict(lasso.fit, newdata = test)
lasso.mse <- mean((y_test - lasso.predict)^2)
lasso.mse
```

The selected tuning parameter lambda is `r lasso.fit$bestTune$lambda`.

**When the 1SE rule is applied, how many predictors are included in the model?**

```{r 1SE lasso}
set.seed(2024)
ctrl2 <- trainControl(method = "cv", number = 10, selectionFunction = "oneSE")
lasso.1se <- train(Sale_Price ~ .,
                   data = train,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1, 
                                          lambda = exp(seq(8, -2, length = 51))),
                   trControl = ctrl2)
plot(lasso.1se, xTrans = log)

# tuning parameter
lasso.1se$bestTune

# coefficients in the final model
coef(lasso.1se$finalModel, lasso.1se$bestTune$lambda)
num_1se <- sum(coef(lasso.1se$finalModel, lasso.1se$bestTune$lambda)[-1, ] != 0)
num_1se
```

There are `r num_1se` nonzero coefficients when the 1SE rule is applied.

**(b) Fit an elastic net model on the training data. Report the selected tuning parameters and the test error. **

```{r caret enet}
set.seed(2024)
enet.caret.fit <- train(Sale_Price ~ .,
                   data = train,
                   method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                                         lambda = exp(seq(8, -2, length = 100))),
                  trControl = ctrl1)
enet.caret.fit$bestTune

myCol <- rainbow(25)
myPar <- list(superpose.symbol = list(col = myCol),
              superpose.line = list(col = myCol))

plot(enet.caret.fit, par.settings = myPar)

# coefficients in the final model
coef(enet.caret.fit$finalModel, enet.caret.fit$bestTune$lambda)

# test error
enet.caret.predict <- predict(enet.caret.fit, newdata = test)
enet.caret.mse <- mean((y_test - enet.caret.predict)^2)
enet.caret.mse
```

The best parameter is when alpha = `r enet.caret.fit$bestTune$alpha` and lambda = `r enet.caret.fit$bestTune$lambda`.

**Is it possible to apply the 1SE rule to select the tuning parameters for elastic net? **

**If the 1SE rule is applicable, implement it to select the tuning parameters. If not, explain why**

```{r 1SE enet}
set.seed(2024)
enet.caret.1se <- train(Sale_Price ~ .,
                   data = train,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1, 
                                          lambda = exp(seq(8, -2, length = 51))),
                   trControl = ctrl2)
plot(enet.caret.1se, xTrans = log)

# tuning parameter
enet.caret.1se$bestTune

# coefficients in the final model
coef(enet.caret.1se$finalModel, enet.caret.1se$bestTune$lambda)
num_1se_enet_caret <- sum(coef(enet.caret.1se$finalModel, enet.caret.1se$bestTune$lambda)[-1, ] != 0)
num_1se_enet_caret
```

There are `r num_1se_enet_caret` nonzero coefficients when the 1SE rule is applied.

**(c) Fit a partial least squares model on the training data and report the test error. **

**How many components are included in your model? **

```{r pls}
set.seed(2024)
pls.fit <- train(x, y,
                 method = "pls",
                 tuneGrid = data.frame(ncomp = 1:19),
                 trControl = ctrl1,
                 preProcess = c("center", "scale"))

ggplot(pls.fit, highlight = TRUE)
pls.fit$bestTune

# test error
pls.predict <- predict(pls.fit, newdata = x_test)
pls.mse = mean((y_test - pls.predict)^2)
pls.mse
```

The the test error of pls model is `r pls.mse`.

There are `r pls.fit$bestTune` components are included in my pls model.

**(d) Choose the best model for predicting the response and explain your choice.**

```{r compare models}
rs <- resamples(list(lasso = lasso.fit, enet = enet.caret.fit, pls = pls.fit))
summary(rs)
bwplot(rs, metric = "RMSE")
parallelplot(rs, metric = "RMSE")
```

The elastic net model since the test error has smaller MAE and RMSE.

However, partial least squares model has higher Rsquared, but the difference is very slight.

Overall, I will choose elastic net model.

**(e) If “caret” was used for the elastic net in (b), retrain this model with “tidymodels”, and vice versa. **

```{r tidymodel enet}
set.seed(2024)
cv_folds <- vfold_cv(train, v = 10) 

enet_spec <- linear_reg(penalty = tune(), mixture = tune()) %>% 
  set_engine("glmnet") %>% 
  set_mode("regression")

# enet_spec %>% extract_parameter_dials("mixture")

enet_grid_set <- parameters(penalty(range = c(-2, 8), trans = log_trans()),
                            mixture(range = c(0, 1)))
enet_grid <- grid_regular(enet_grid_set, levels = c(100, 21))



enet_workflow <- workflow() %>%
  add_model(enet_spec) %>%
  add_formula(Sale_Price ~ .)

enet_tune <- tune_grid(
  enet_workflow,
  resamples = cv_folds,
  grid = enet_grid
)

autoplot(enet_tune, metric = "rmse") + 
  theme(legend.position = "top") +
  labs(color = "Mixing Percentage\n(Alpha Values)") 

enet_best <- select_best(enet_tune, metric = "rmse") 

final_enet_spec <- enet_spec %>% 
  update(penalty = enet_best$penalty, mixture = enet_best$mixture)

enet_fit <- fit(final_enet_spec, formula = Sale_Price ~ ., data = train)

# Get coefficients
enet_model <- extract_fit_engine(enet_fit)
coef(enet_model, s = enet_best$penalty)

num_enet <- sum(coef(enet_model, s = enet_best$penalty)[-1, ] != 0)
num_enet
```

There are `r num_enet` nonzero coefficients when the 1SE rule is applied, which is `r num_enet-num_1se_enet_caret` more than the caret model.

**Compare the selected tuning parameters between the two software approaches. **

**Should there be discrepancies in the chosen parameters, discuss potential reasons for these differences.**

```{r}
enet_best
enet.caret.fit$bestTune
```

The tuning parameter of tidymodel are `r enet_best`, while that of caret model are `r enet.caret.fit$bestTune`.

The alpha is the same, but the lambda have a slightly difference. It may because that `caret` and `tidymodels` may implement the Elastic Net algorithm slightly differently under the hood, leading to discrepancies in the optimal parameters found. Different preprocessing steps could lead to models with different parameters being selected as optimal.
