---
title: "Dimension Reduction Methods in Linear Regression"
author: "Yifei Sun, Runze Cui"
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
header-includes:
- \usepackage{fancyhdr}
- \usepackage{lipsum}
- \pagestyle{fancy}
- \fancyhead[R]{\thepage}
- \fancypagestyle{plain}{\pagestyle{fancy}}
editor_options: 
  chunk_output_type: console
--- 

\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo = T, message = FALSE, results='hide', warning=FALSE}
library(ISLR)
library(pls)
library(caret)
library(tidymodels)
```

Predict a baseball playerâ€™s salary on the basis of various statistics associated with performance in the previous year. Use `?Hitters` for more details. 

```{r}
data(Hitters)
Hitters <- na.omit(Hitters)
set.seed(2222)

data_split <- initial_split(Hitters, prop = 0.8)

# Extract the training and test data
training_data <- training(data_split)
testing_data <- testing(data_split)
```

```{r}
# training data
x <- model.matrix(Salary ~ ., training_data)[, -1]
y <- training_data$Salary

# test data
x2 <- model.matrix(Salary ~ .,testing_data)[, -1]
y2 <- testing_data$Salary
```

# SVD 

```{r}
# center and scale
x3 <- scale(x)

# SVD
x_svd <- svd(x3)
u <- x_svd$u
v <- x_svd$v
d <- diag(x_svd$d)
# corrplot::corrplot(t(u) %*% u, is.corr = FALSE)
# corrplot::corrplot(t(v) %*% v, is.corr = FALSE)
# corrplot::corrplot(v %*% t(v), is.corr = FALSE)
# corrplot::corrplot(d, is.corr = FALSE)

# definition
x4 <- u %*% d %*% t(v)
all.equal(x3, x4, check.attributes = FALSE)

# PCA
x_pca <- prcomp(x, scale. = TRUE)

all.equal(x_pca$rotation, v, check.attributes = FALSE)
```

# Principal components regression (PCR)

We fit the PCR model using the function `pcr()`.

```{r}
set.seed(2)
pcr.mod <- pcr(Salary ~ ., 
               data = training_data,
               scale = TRUE, # scale = FALSE by default
               validation = "CV")

summary(pcr.mod)

validationplot(pcr.mod, val.type = "MSEP", legendpos = "topright")

cv.mse <- RMSEP(pcr.mod)
ncomp.cv <- which.min(cv.mse$val[1,,]) - 1
ncomp.cv

predy2.pcr <- predict(pcr.mod, newdata = testing_data, 
                      ncomp = ncomp.cv)
# test MSE
mean((y2 - predy2.pcr)^2)
```


# Partial least squares (PLS)

We fit the PLS model using the function `plsr()`.
```{r}
set.seed(2)
pls.mod <- plsr(Salary~., 
                data = training_data, 
                scale = TRUE,  
                validation = "CV")

summary(pls.mod)
validationplot(pls.mod, val.type = "MSEP", legendpos = "topright")

cv.mse <- RMSEP(pls.mod)
ncomp.cv <- which.min(cv.mse$val[1,,]) - 1
ncomp.cv

predy2.pls <- predict(pls.mod, newdata = testing_data, 
                      ncomp = ncomp.cv)
# test MSE
mean((y2 - predy2.pls)^2)
```

# PCR and PLS using `caret`

## PCR 

```{r}
ctrl1 <- trainControl(method = "repeatedcv", 
                      number = 10,
                      repeats = 5,
                      selectionFunction = "best") # "oneSE" for the 1SE rule

# show information about the model
modelLookup("pcr")
modelLookup("pls")

# Two ways for standardizing predictors

# train(..., preProc = c("center", "scale"))
set.seed(2)
pcr.fit <- train(x, y,
                 method = "pcr",
                 tuneGrid  = data.frame(ncomp = 1:19),
                 trControl = ctrl1,
                 preProcess = c("center", "scale"))

predy2.pcr2 <- predict(pcr.fit, newdata = x2)
mean((y2 - predy2.pcr2)^2)

# pcr(..., scale = TRUE)
set.seed(2)
pcr.fit2 <- train(x, y,
                  method = "pcr",
                  tuneGrid = data.frame(ncomp = 1:19),
                  trControl = ctrl1,
                  scale = TRUE)

predy2.pcr3 <- predict(pcr.fit2, newdata = x2)
mean((y2 - predy2.pcr3)^2)

ggplot(pcr.fit, highlight = TRUE) + theme_bw()
```

## PLS

```{r}
set.seed(2)
pls.fit <- train(x, y,
                 method = "pls",
                 tuneGrid = data.frame(ncomp = 1:19),
                 trControl = ctrl1,
                 preProcess = c("center", "scale"))
predy2.pls2 <- predict(pls.fit, newdata = x2)
mean((y2 - predy2.pls2)^2)

ggplot(pls.fit, highlight = TRUE)
```

Here are some old code on elastic net.

```{r}
set.seed(2)
enet.fit <- train(Salary ~ .,
                  data = training_data,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                                         lambda = exp(seq(6, 0, length = 100))),
                  trControl = ctrl1)

# myCol <- rainbow(25)
# myPar <- list(superpose.symbol = list(col = myCol),
#               superpose.line = list(col = myCol))
# plot(enet.fit, xTrans = log, par.settings = myPar)
```

Comparing the models based on resampling results.

```{r}
resamp <- resamples(list(elastic_net = enet.fit, 
                         pcr = pcr.fit, 
                         pls = pls.fit))
summary(resamp)

bwplot(resamp, metric = "RMSE")
```


