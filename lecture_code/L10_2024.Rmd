---
title: "Regression Trees and Classification Trees"
author: "Yifei Sun"
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
header-includes:
- \usepackage{fancyhdr}
- \usepackage{lipsum}
- \pagestyle{fancy}
- \fancyhead[R]{\thepage}
- \fancypagestyle{plain}{\pagestyle{fancy}}
--- 

\newpage


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
```

```{r, echo = T, message = FALSE, results='hide', warning=FALSE}
library(ISLR)
library(mlbench)
library(caret)
library(tidymodels)
library(rpart)
library(rpart.plot)
library(party)
library(partykit)
library(pROC)
```



# Regression Trees

Predict a baseball player’s salary on the basis of various statistics associated with performance in the previous year. Use `?Hitters` for more details.

```{r}
data(Hitters)
Hitters <- na.omit(Hitters)

set.seed(2)
data_split <- initial_split(Hitters, prop = 0.8)

# Extract the training and test data
training_data <- training(data_split)
testing_data <- testing(data_split)
```

## The CART approach

We first apply the regression tree method to the Hitters data. `cp` is the complexity parameter. The default value for `cp` is 0.01. Sometimes the default value may over prune the tree.

```{r}
set.seed(1)
tree1 <- rpart(formula = Salary ~ . , 
               data = training_data,
               control = rpart.control(cp = 0))

## plot.rpart
# plot(tree1)
# text(tree1)

rpart.plot(tree1)
```

We get a smaller tree by increasing the complexity parameter.
```{r, fig.height=3, fig.width=4}
set.seed(1)
tree2 <- rpart(Salary ~ . ,  
               data = training_data,
               control = rpart.control(cp = 0.1))
rpart.plot(tree2)
```


We next apply cost complexity pruning to obtain a tree with the right size. The functions `printcp()` and `plotcp()` give the set of possible cost-complexity prunings of a tree from a nested set. For the geometric means of the intervals of values of `cp` for which a pruning is optimal, a cross-validation has been done in the initial construction by `rpart()`. 

The `cptable` in the fit contains the mean and standard deviation of the errors in the cross-validated prediction against each of the geometric means, and these are plotted by `plotcp()`. `Rel error` (relative error) is `\(1 – R^2\)`. The x-error is the cross-validation error generated by built-in cross validation. A good choice of `cp` for pruning is often the leftmost value for which the mean lies below the horizontal line.

```{r}
printcp(tree1)
cpTable <- tree1$cptable
plotcp(tree1)
```

Prune the tree based on the `cp` table.

```{r}
# minimum cross-validation error
minErr <- which.min(cpTable[,4])
tree3 <- rpart::prune(tree1, cp = cpTable[minErr,1])
rpart.plot(tree3)
```

```{r, fig.height=6, fig.width=12}
plot(as.party(tree3))
summary(tree3)
```


```{r}
# 1SE rule
tree4 <- rpart::prune(tree1, 
                      cp = cpTable[cpTable[,4]<cpTable[minErr,4]+cpTable[minErr,5],1][1])
rpart.plot(tree4)
```

Finally, the function `predict()` can be used for prediction from a fitted `rpart` object.

```{r}
head(predict(tree3, newdata = testing_data))
```


## Conditional inference trees

The implementation utilizes a unified framework for conditional inference, or permutation tests. Unlike CART, the stopping criterion is based on p-values. A split is implemented when (1 - p-value) exceeds the value given by `mincriterion` as specified in `ctree_control()`. This approach ensures that the right-sized tree is grown without additional pruning or cross-validation, but can stop early. At each step, the splitting variable is selected as the input variable with strongest association to the response (measured by a p-value corresponding to a test for the partial null hypothesis of a single input variable and the response). Such a splitting procedure can avoid a variable selection bias towards predictors with many possible cutpoints.

```{r, fig.height=5, fig.width=15}
tree5 <- ctree(Salary ~ . , 
               training_data)
plot(tree5)
```

Note that `tree5` is a `party` object. The function `predict()` can be used for prediction from a fitted `party` object.

```{r}
head(predict(tree5, newdata = testing_data))
```

## `caret`


```{r}
ctrl <- trainControl(method = "cv")

set.seed(1)
rpart.fit <- train(Salary ~ . , 
                   training_data, 
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-6,-2, length = 100))),
                   trControl = ctrl)
plot(rpart.fit, xTrans = log)
rpart.plot(rpart.fit$finalModel)
```

We can also fit a conditional inference tree model. The tuning parameter is `mincriterion`.

```{r}
set.seed(1)
ctree.fit <- train(Salary ~ . , 
                   training_data, 
                   method = "ctree",
                   tuneGrid = data.frame(mincriterion = 1-exp(seq(-6, -2, length = 100))),
                   trControl = ctrl)
ggplot(ctree.fit, highlight = TRUE)
```

```{r, fig.height=5, fig.width=15}
plot(ctree.fit$finalModel)
```


```{r}
summary(resamples(list(rpart.fit, ctree.fit)))
```

```{r}
RMSE(predict(rpart.fit, newdata = testing_data), testing_data$Salary)
RMSE(predict(ctree.fit, newdata = testing_data), testing_data$Salary)
```


## `tidymodels`

```{r}
set.seed(2)
cv_folds <- vfold_cv(training_data, v = 10)

# Model specification
rpart_spec <- decision_tree(cost_complexity = tune(), tree_depth = 30, min_n = 20) %>%
  set_engine("rpart") %>%
  set_mode("regression")

# Tuning grid
rpart_grid_set <- dials::parameters(cost_complexity(range = c(-6, -2), trans = log_trans()))
rpart_grid <- grid_regular(rpart_grid_set, levels = c(100))

# Set up the workflow
rpart_workflow <- workflow() %>%
  add_model(rpart_spec) %>%
  add_formula(Salary ~ .)

rpart_tune <- rpart_workflow %>%
  tune_grid(resamples = cv_folds,
            grid = rpart_grid)

autoplot(rpart_tune, metric = "rmse")
rpart_best <- select_best(rpart_tune, metric = "rmse")

# Update the model spec
final_rpart_spec <- rpart_spec %>% 
  update(cost_complexity = rpart_best$cost_complexity)

rpart_fit <- parsnip::fit(final_rpart_spec, formula = Salary ~ ., data = training_data)
rpart.plot(rpart_fit$fit, roundint = FALSE)
```

# Classification trees

We use the Pima Indians Diabetes Database for illustration. The data contain 768 observations and 9 variables. The outcome is a binary variable `diabetes`. 

```{r}
data(PimaIndiansDiabetes2)
dat <- PimaIndiansDiabetes2
# dat$diabetes <- factor(dat$diabetes, c("pos", "neg"))

dat <- na.omit(PimaIndiansDiabetes2)

set.seed(1)
data_split <- initial_split(dat, prop = 0.7)

# Extract the training and test data
training_data <- training(data_split)
testing_data <- testing(data_split)
```


## `rpart`

```{r}
set.seed(1)
tree1 <- rpart(formula = diabetes ~ . , 
               data = training_data, 
               control = rpart.control(cp = 0))

cpTable <- printcp(tree1)
plotcp(tree1)

# minimum cross-validation error; may also use the 1SE rule
minErr <- which.min(cpTable[,4])
tree2 <- rpart::prune(tree1, cp = cpTable[minErr,1])
rpart.plot(tree2)
# summary(tree2)
```

## `ctree`

```{r, fig.height=6, fig.width=18}
tree2 <- ctree(formula = diabetes ~ . , 
               data = training_data)
plot(tree2)
```
 
## `caret`

### CART

```{r}
ctrl <- trainControl(method = "cv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)

set.seed(1)
rpart.fit <- train(diabetes ~ . , 
                   training_data,
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-8,-3, len = 100))),
                   trControl = ctrl,
                   metric = "ROC")
plot(rpart.fit, xTrans = log)
rpart.plot(rpart.fit$finalModel)
```

### CIT

```{r}
set.seed(1)
ctree.fit <- train(diabetes ~ . ,
                   training_data,
                   method = "ctree",
                   tuneGrid = data.frame(mincriterion = 1-exp(seq(-2, -0.2, length = 100))),
                   metric = "ROC",
                   trControl = ctrl)
ggplot(ctree.fit, highlight = TRUE)
```

```{r, fig.width=20, fig.height=8}
plot(ctree.fit$finalModel)
summary(resamples(list(rpart.fit, ctree.fit)))
```



```{r}
rpart.pred <- predict(tree1, newdata = testing_data)[,1]

rpart.pred2 <- predict(rpart.fit, newdata = testing_data,
                       type = "prob")[,1]

ctree.pred <- predict(ctree.fit, newdata = testing_data,
                       type = "prob")[,1]

roc.rpart <- roc(testing_data$diabetes, rpart.pred2)
roc.ctree <- roc(testing_data$diabetes, ctree.pred)

auc <- c(roc.rpart$auc[1], roc.ctree$auc[1])

plot(roc.rpart, legacy.axes = TRUE)
plot(roc.ctree, col = 2, add = TRUE)


modelNames <- c("rpart","ctree")
legend("bottomright", legend = paste0(modelNames, ": ", round(auc,3)),
       col = 1:2, lwd = 2)
```


## `tidymodels`

```{r}
set.seed(2)
cv_folds <- vfold_cv(training_data, v = 10)

# Model specification
rpart_spec <- decision_tree(cost_complexity = tune(), tree_depth = 30, min_n = 20) %>%
  set_engine("rpart") %>%
  set_mode("classification")

# Tuning grid
rpart_grid_set <- dials::parameters(cost_complexity(range = c(-8, -3), trans = log_trans()))
rpart_grid <- grid_regular(rpart_grid_set, levels = c(100))

# Set up the workflow
rpart_workflow <- workflow() %>%
  add_model(rpart_spec) %>%
  add_formula(diabetes ~ .)

rpart_tune <- rpart_workflow %>%
  tune_grid(resamples = cv_folds,
            grid = rpart_grid)

autoplot(rpart_tune, metric = "roc_auc")
rpart_best <- select_best(rpart_tune, metric = "roc_auc")

# Update the model spec
final_rpart_spec <- rpart_spec %>% 
  update(cost_complexity = rpart_best$cost_complexity)

rpart_fit <- parsnip::fit(final_rpart_spec, formula = diabetes ~ ., data = training_data)
rpart.plot(rpart_fit$fit, roundint = FALSE)
```


